{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytextrank\n",
    "- blog: https://chaeeunsong.tistory.com/25\n",
    "- .py에서는 결과가 나오는데, ipynb로 하면 결과가 안나왔음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import spacy\n",
    "import pytextrank\n",
    "# scapy의 한국어모델인 ko_core_news_sm을 사용함 https://spacy.io/models/ko/\n",
    "# example text\n",
    "text = \"서울 지진 피해에 대한 데이터 분석을 위해서는 어떤 종류의 데이터를 사용해야 할지 먼저 생각해보아야 합니다. 예를 들어, 지진 발생 시간, 지진 규모, 지진 발생 지역, 피해 규모 등의 정보가 필요할 것입니다. 서울 지진 피해 분석 예시: 서울 지역에서 최근 몇 년간 발생한 지진 데이터를 수집하여 지진 발생 건수, 지진 규모, 지진 발생 지역 등의 정보를 파악할 수 있습니다. 이를 바탕으로 서울 지역에서 지진 발생이 가장 많은 지역, 지진 규모와 피해 규모 간의 상관 관계, 지진 발생 시간대 등을 분석할 수 있습니다. 또한, 특정 지역에서의 지진 발생 시 피해 규모가 어떻게 나타나는지 분석하여 지진 대비 대응 전략을 마련할 수 있습니다. 서울 지진에 대한 데이터는 국가지진정보센터에서 제공하는 '국내 지진 정보 시스템'에서 확인할 수 있습니다. 이 시스템에서는 지난 1년간의 국내 지진 정보를 확인할 수 있으며, 서울 지역에서 발생한 지진 정보도 포함되어 있습니다. 이를 바탕으로 데이터를 수집하고 분석할 수 있습니다.\"\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"ko_core_news_sm\")\n",
    "\n",
    "#add PyTextRank to the spaCy pipeline\n",
    "doc = nlp(text)\n",
    "print(doc._.phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "nlp.add_pipe(\"textrank\")\n",
    "\n",
    "keywords_textrank = []\n",
    "keywords_textrank_rank = []\n",
    "for phrase in doc._.phrases[:10]:\n",
    "    keywords_textrank.append(phrase.text)\n",
    "    keywords_textrank_rank.append(phrase.rank)\n",
    "print(keywords_textrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# topicRank\n",
    "nlp.add_pipe(\"topicrank\")\n",
    "\n",
    "doc = nlp(text)\n",
    "print(len(doc))\n",
    "\n",
    "keywords_topicrank = []\n",
    "keywords_topicrank_rank = []\n",
    "for phrase in doc._.phrases[:10]:\n",
    "    keywords_topicrank.append(phrase.text)\n",
    "    keywords_topicrank_rank.append(phrase.rank)\n",
    "\n",
    "print(keywords_topicrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                          Text      Root Text       Root Dep      Root Head Text\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "[E894] The 'noun_chunks' syntax iterator is not implemented for language 'ko'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(str_format\u001b[39m.\u001b[39mformat(\u001b[39m'\u001b[39m\u001b[39mText\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRoot Text\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRoot Dep\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRoot Head Text\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m40\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39mnoun_chunks:\n\u001b[0;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(str_format\u001b[39m.\u001b[39mformat(chunk\u001b[39m.\u001b[39mtext, chunk\u001b[39m.\u001b[39mroot\u001b[39m.\u001b[39mtext, chunk\u001b[39m.\u001b[39mroot\u001b[39m.\u001b[39mdep_, chunk\u001b[39m.\u001b[39mroot\u001b[39m.\u001b[39mhead\u001b[39m.\u001b[39mtext))\n",
      "File \u001b[1;32mc:\\Users\\SONG\\Desktop\\GIT\\juso-chat-search\\chat_venv\\lib\\site-packages\\spacy\\tokens\\doc.pyx:865\u001b[0m, in \u001b[0;36mnoun_chunks\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: [E894] The 'noun_chunks' syntax iterator is not implemented for language 'ko'."
     ]
    }
   ],
   "source": [
    "# noun_chunks는 한글 지원안됨\n",
    "noun_chunks = doc.noun_chunks\n",
    "\n",
    "print(\"==\"*40)\n",
    "str_format = \"{:>30}{:>15}{:>15}{:>20}\"\n",
    "print(str_format.format('Text', 'Root Text', 'Root Dep', 'Root Head Text'))\n",
    "print(\"==\"*40)\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(str_format.format(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
